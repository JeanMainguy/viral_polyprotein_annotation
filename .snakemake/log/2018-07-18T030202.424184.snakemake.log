Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	extract_viral_protein
	1

rule extract_viral_protein:
    input: data/taxonomy/taxonomy_virus.txt
    output: data/viral_proteins/Viruses_protein_db.faa, results/stat_viral_protein/stat_proteins_Viruses.csv
    jobid: 0

    Error in rule extract_viral_protein:
        jobid: 0
        output: data/viral_proteins/Viruses_protein_db.faa, results/stat_viral_protein/stat_proteins_Viruses.csv

RuleException:
CalledProcessError in line 19 of /proj/viral_polyprotein_annotation/Snakefile:
Command ' set -euo pipefail;  bash scripts/extraction_viral_proteins.sh ' returned non-zero exit status 127.
  File "/proj/viral_polyprotein_annotation/Snakefile", line 19, in __rule_extract_viral_protein
  File "/apps/python3/3.6.3/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /proj/viral_polyprotein_annotation/.snakemake/log/2018-07-18T030202.424184.snakemake.log
